
@article{krafczyk_learning_2021,
	title = {Learning from reproducing computational results: introducing three principles and the {Reproduction} {Package}},
	volume = {379},
	shorttitle = {Learning from reproducing computational results},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsta.2020.0069},
	doi = {10.1098/rsta.2020.0069},
	abstract = {We carry out efforts to reproduce computational results for seven published articles and identify barriers to computational reproducibility. We then derive three principles to guide the practice and dissemination of reproducible computational research: (i) Provide transparency regarding how computational results are produced; (ii) When writing and releasing research software, aim for ease of (re-)executability; (iii) Make any code upon which the results rely as deterministic as possible. We then exemplify these three principles with 12 specific guidelines for their implementation in practice. We illustrate the three principles of reproducible research with a series of vignettes from our experimental reproducibility work. We define a novel Reproduction Package, a formalism that specifies a structured way to share computational research artifacts that implements the guidelines generated from our reproduction efforts to allow others to build, reproduce and extend computational science. We make our reproduction efforts in this paper publicly available as exemplar Reproduction Packages.

This article is part of the theme issue ‘Reliability and reproducibility in computational science: implementing verification, validation and uncertainty quantification in silico’.},
	number = {2197},
	urldate = {2024-05-10},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Krafczyk, M. S. and Shi, A. and Bhaskar, A. and Marinov, D. and Stodden, V.},
	month = mar,
	year = {2021},
	note = {Publisher: Royal Society},
	keywords = {reproducibility, code packaging, open code, open data, software testing, verification},
	pages = {20200069},
	file = {Full Text PDF:/home/amy/Zotero/storage/J8BX7DB9/Krafczyk et al. - 2021 - Learning from reproducing computational results i.pdf:application/pdf},
}

@article{wood_replication_2018,
	title = {Replication {Protocol} for {Push} {Button} {Replication} ({PBR})},
	url = {https://osf.io/yfbr8/},
	doi = {https://doi.org/10.17605/OSF.IO/YFBR8},
	abstract = {3ie’s Replication Programme is conducting a PBRproject.PBR studies test the ability for another researcher to use data and code to reproduce the originally published results.PBR researchersare not tasked with evaluating the quality of the original research or testing the robustness of the original results to any type of sensitivity analysis. They are also not expected to explore any original coding decisions. When conducting PBRstudies, 3ie requires researchers to follow a set protocol. The steps of the protocol are listed sequentially and outlined below.Each paper subject to replication will have a unique component in the Open Science Framework (OSF)platform.},
	language = {en-us},
	urldate = {2024-05-10},
	journal = {OSF},
	author = {Wood, Benjamin and Brown, Annette and Djimeu, Eric and Vasquez, Maria and Yoon, Semi and Burke, Jane},
	month = jan,
	year = {2018},
	note = {Publisher: Open Science Framework},
	file = {Wood - 2018 - Replication Protocol for Push Button Replication (.pdf:/home/amy/Zotero/storage/ITZP3QMT/Wood - 2018 - Replication Protocol for Push Button Replication (.pdf:application/pdf},
}

@misc{haroz_comparison_2022,
	title = {Comparison of {Preregistration} {Platforms}},
	url = {https://osf.io/preprints/metaarxiv/zry2u},
	doi = {https://doi.org/10.31222/osf.io/zry2u},
	abstract = {Preregistration can force researchers to front-load a lot of decision-making to an early stage of a project. Choosing which preregistration platform to use must be therefore be one of those early decisions, and because a preregistration cannot be moved, that choice is permanent. This article aims to help researchers who are already interested in preregistration choose a platform by clarifying differences between them. Preregistration criteria and features are explained and analyzed for sites that cater to a broad range of research fields, including: GitHub, AsPredicted, Zenodo, the Open Science Framework (OSF), and an “open-ended” variant of OSF. While a private prespecification document can help mitigate self-deception, this guide considers publicly shared preregistrations that aim to improve credibility. It therefore defines three of the criteria (a timestamp, a registry, and persistence) as a bare minimum for a valid and reliable preregistration. GitHub and AsPredicted fail to meet all three. Zenodo and OSF meet the basic criteria and vary in which additional features they offer.},
	urldate = {2024-05-10},
	publisher = {MetaArXiv},
	author = {Haroz, Steve},
	month = feb,
	year = {2022},
	file = {Haroz - Comparison of Preregistration Platforms.pdf:/home/amy/Zotero/storage/W7GW7B4K/Haroz - Comparison of Preregistration Platforms.pdf:application/pdf},
}

@article{wood_push_2018,
	title = {Push button replication: {Is} impact evaluation evidence for international development verifiable?},
	volume = {13},
	issn = {1932-6203},
	shorttitle = {Push button replication},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0209416},
	doi = {10.1371/journal.pone.0209416},
	abstract = {Objective Empirical research that cannot be reproduced using the original dataset and software code (replication files) creates a credibility challenge, as it means those published findings are not verifiable. This study reports the results of a research audit exercise, known as the push button replication project, that tested a sample of studies that use similar empirical methods but span a variety of academic fields. Methods We developed and piloted a detailed protocol for conducting push button replication and determining the level of comparability of these replication findings to original findings. We drew a sample of articles from the ten journals that published the most impact evaluations from low- and middle-income countries from 2010 through 2012. This set includes health, economics, and development journals. We then selected all articles in these journals published in 2014 that meet the same inclusion criteria and implemented the protocol on the sample. Results Of the 109 articles in our sample, only 27 are push button replicable, meaning the provided code run on the provided dataset produces comparable findings for the key results in the published article. The authors of 59 of the articles refused to provide replication files. Thirty of these 59 articles were published in journals that had replication file requirements in 2014, meaning these articles are non-compliant with their journal requirements. For the remaining 23 of the 109 articles, we confirmed that three had proprietary data, we received incomplete replication files for 15, and we found minor differences in the replication results for five. Conclusion The findings presented here reveal that many economics, development, and public health researchers are a long way from adopting the norm of open research. Journals do not appear to be playing a strong role in ensuring the availability of replication files.},
	language = {en},
	number = {12},
	urldate = {2024-05-10},
	journal = {PLOS ONE},
	author = {Wood, Benjamin D. K. and Müller, Rui and Brown, Annette N.},
	month = dec,
	year = {2018},
	note = {Publisher: Public Library of Science},
	keywords = {Health economics, Development economics, Economic development, Medical journals, Open data, Public and occupational health, Replication studies, Scientific publishing},
	pages = {e0209416},
	file = {Full Text PDF:/home/amy/Zotero/storage/UFWQJ7I8/Wood et al. - 2018 - Push button replication Is impact evaluation evid.pdf:application/pdf},
}

@article{monks_computer_2023,
	title = {Computer model and code sharing practices in healthcare discrete-event simulation: a systematic scoping review},
	volume = {0},
	issn = {1747-7778},
	shorttitle = {Computer model and code sharing practices in healthcare discrete-event simulation},
	url = {https://doi.org/10.1080/17477778.2023.2260772},
	doi = {10.1080/17477778.2023.2260772},
	abstract = {Discrete-event simulation (DES) is a widely used computational method in health services and health economic studies. This scoping review investigates to what extent authors share DES models and audits if sharing adheres to best practice. The Web of Science, Scopus, PubMed, and ACM Digital Library databases were searched between January 1 2019 till December 31 2022. Cost-effectiveness, health service research and methodology studies in a health context were included. Data extraction and audit were performed by two reviewers. We measured the proportion of literature that shared models; we report analyses by publication type, year of publication, COVID-19 application; and free and open source versus commercial software. Out of the 564 studies included, 47 (8.3\%) cited a published computer model, rising to 9.0\% in 2022. Studies were more likely to share models if they had been developed using free and open source tools. Studies rarely followed best practice when sharing computer models. Although still in the minority, healthcare DES authors are increasingly sharing their computer model artefacts. Although commercial software dominates the DES literature, free and open source software plays a crucial role in sharing. The DES community can adopt simple best practices to improve the quality of sharing.},
	number = {0},
	urldate = {2024-05-10},
	journal = {Journal of Simulation},
	author = {Monks, Thomas and Harper, Alison},
	year = {2023},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/17477778.2023.2260772},
	keywords = {open science, reproducibility, review, healthcare, Discrete-event simulation, sharing},
	pages = {1--16},
	file = {Full Text PDF:/home/amy/Zotero/storage/KDBG4H9R/Monks and Harper - 2023 - Computer model and code sharing practices in healt.pdf:application/pdf},
}

@article{monks_towards_2024,
	title = {Towards sharing tools and artefacts for reusable simulations in healthcare},
	volume = {0},
	issn = {1747-7778},
	url = {https://doi.org/10.1080/17477778.2024.2347882},
	doi = {10.1080/17477778.2024.2347882},
	abstract = {Discrete-event simulation (DES) is a widely used computational method in health services and health economic studies. Despite increasing recognition of the advantages of open, reusable DES models for both healthcare practitioners and simulation researchers, in practice very few authors share their model code alongside a published paper. In the context of Free and Open Source Software (FOSS), this paper presents a pilot framework called STARS: Sharing Tools and Artefacts for Reusable Simulations to begin to address the challenges and leverage the opportunities of sharing DES models in healthcare. STARS aligns with existing guidelines and documentation, including reproducibility initiatives, and enables computer models to be shared with users of differing technical abilities. We demonstrate the feasibility and applicability of STARS with three applied DES examples using Python. Our framework supports the development of open, reusable DES models which can enable partner healthcare organisations to preview, validate, and use models. Academic research teams can benefit from knowledge exchange, enhanced recognition and scrutiny of their work, and long-term archiving of models.},
	number = {0},
	urldate = {2024-05-13},
	journal = {Journal of Simulation},
	author = {Monks, Thomas and Harper, Alison and Mustafee, Navonil},
	year = {2024},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/17477778.2024.2347882},
	keywords = {Discrete-event simulation, healthcare, open science, reusable models},
	pages = {1--20},
	file = {Full Text PDF:/home/amy/Zotero/storage/V3EYVUHB/Monks et al. - 2024 - Towards sharing tools and artefacts for reusable s.pdf:application/pdf},
}
