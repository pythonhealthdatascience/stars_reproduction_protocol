\section{Introduction to the STARS project}

This protocol is part of the project STARS: ”Sharing Tools and Artefacts for Reusable Simulations in healthcare”. The aim of STARS is to...

\textbf{TO DO:} Write this section. Mention the aim, and refer to the key papers/pilot work, and then how that relates to this protocol (to set the scene). Keep it brief.

\autocite{monks_towards_2024}

\section{Selection of simulation models}

This protocol will be used within two work packages on STARS:
\begin{itemize}
    \item Work package 1 - to reproduce six published discrete event simulation models by external authors
    \item Work package 3 - to reproduce two simulation models produced by/in collaboration with members of the STARS team
\end{itemize}

\textbf{TO DO:} Remove mention of work packages, and just focus on the work being done (as work packages just relevant to us, not external) as that would make this clearer

For work package 1, the six models will be chosen from those identified by Monks and Harper 2023\autocite{monks_computer_2023} in their paper "Computer model and code sharing practices in healthcare discrete-event simulation: a systematic scoping review". Their review of discrete-event simulation models in healthcare identified 47 studies citing an openly available model that was used to generate their results. When selecting six of these models, we have two criteria: (i) The study has an open license (either already published, or added upon request from the STARS team), and (ii) That a balance of Python and R models are chosen.

For work package 3, the models will be developed within or with support from the STARS team, with a team member acting as a holdout to conduct the reproducibility test.

\section{Reproduction of simulation models}

\textbf{Read article...}

Download article and all supplementary and put in folder on GitHub that don't touch

FOCUS ON METHODS and just about recording key details TBH: write summary of what the model is: (a) model creation (packages, software, versions) (b) model flow/diagram (if provided or create) (c) mentioned data used (d) citation to authors

FOCUS ON RESULTS: Define scope of reproduction. This should be:
* All tables and figures within the manuscript that contain results
* Any other key results in the paper (e.g. mentioned abstract, prominent in results)
This must be done before commence any reproduction. Should archive this publicly so can't backtrack on it. Can think about whether this is a one or two person decision (could be a review point)

\textbf{Look at code and data...} 

(does any of this need to be done beforehand to ascertain whether we have anything here? or is that in the selection step?)

Download all there stuff and put in a folder on GitHub that don't touch.
Also make a second copy - that's where I will work on it and edit it.

Create a tree for their repository

Then a structured way of looking through it...
* Make a table recording all data files (name, location, one-sentence summary of what it is)
* Make a table recording all code files (name, location, one-sentence summary of what it does)

Then try and identify which sections of the code will produce your things from your scope

\textbf{Environment...}

Set up environment
* Using provided files
* If nothing provided, look at what they import and make environment using latest version (unless can spot any obvious reason why wouldn't be latest) - i.e. specify no version and let it all download

\textbf{License...}

Add a license to the repository. Needs to be compatabile with:
* The license the author used
* The license of the packages

If not able to use completely permissive license, add note explaining why (e.g. if built in RSimmer, will need to be GPL)

\textbf{Run code}
Try to run the code to produce the items from your scope.
Make it obvious where each item in the scope comes from (perhaps using Rmd/ipynb, perhaps with comments in the files that create it then Rmd to present, or something like that). If possible to do it all in a notebook that's nice as then you have code and stuff all in one place... but guessing might not always be practical...
\textbf{Consider:} Some people convert this into reproducibility package when they try and run it, and end up with the result being a Makefile that runs and produces everything. Do we do that now? Or do we save that for the reproduction package stage, and here, just focus on trying to get it to run and not reworking stuff too much? I think so.

\textbf{Encountering errors...}
Try to resolve errors, able to edit code or make changes freely to try and solve.
It get really stuck, contact authors. Follow the ACRE language guidance (download that with protocol, put in supplemnentary or something, will full citation and their license)

\textbf{Time...}
\textbf{Consider:} Whether have fixed time, or whether it's more subjective of keep going until just get stuck.
More common not to have fixed time, but to have a stopping point that you decide upon. But also could. Not sure.

\textbf{Defining replication success...}
Success should be defined for each item in the scope

For figures: eyeball it, is it similar, if not what are differences, categories like in that example of categories
For numbers (standalone or in tables): perhaps some sort of threshold

Then, look at recommendations on how to decide on overall class of a figure or table (as it could be all good except one thing)

Then, how to define it overall for the paper

\textbf{Keeping a log...}
Keep a daily log of work. Very detailed. Open access.

\textbf{Number of researchers...}
One person does it. Not sure if we want any further people involvement - either full repro, or dedicated check in stages, or more informal. Think the main one is getting agreement on scope.


\subsection{old notes}

The protocol for the reproduction of simulation models is primarily adapted from:
\begin{itemize}
    \item Krafczyk et al. 2021 - "Learning from reproducing computational results: introducing three principles and the Reproduction Package"\autocite{krafczyk_learning_2021}
    \item Wood et al. 2017 - "Replication protocol for push button replication"\autocite{wood_replication_2018} - as from their paper: "Push button replication: Is impact evaluation evidence for international development verifiable?"\autocite{wood_push_2018}
\end{itemize}

Rough notes from Tom (Krafczyk):

Maximum 40 hours

1. Break tables and figures in a DES paper into a set of experiments. --> \textbf{DEFINE what is included using Wood}
2. Survey the model code and data provided with the publication. 
3. Modify and write code to run computational experiments. --> \textbf{refine this using Wood}
4. Automate computational experiments and visualizations. --> \textbf{further details from Krafczyk and Wood}
5. Restructure each step into a reproducible software package.
6. Track issues, barriers, and enablers to reuse and reproduction. --> \textbf{under record keeping}
7. Produce a plot reporting the percentage of experiments reproduced versus time. --> \textbf{under reporting, as this is just one figure that covers all of the paper}

To further support reproducibility, we will adopt a clear definition of replication success for a stochastic model using McManus et al (2021). We will investigate if conclusions remain the same allowing for multiple percent deviations from those reported (e.g 5, 10, 20 percent). --> \textbf{appraise this suggestion - this is more for papers where they are trying to get key figures - can have different rules for different types of output, as in examples in my notes}

We will adopt the same approach to measuring time (i.e. what activities are included and excluded) as used in Krafczyk et al (2021). --> \textbf{refine this using Wood}

\section{Reproduction packages}

\textbf{TO DO:} Write this section

Look at:

Krafczyk reproduction package

ACM RCR replication package


\section{Record keeping}

\textbf{TO DO:} Write this section

Rough notes: TRACE modelling notebook (cav: designed for making models and then translating to TRACE reporting), STRESS reporting guidelines

TRACE:

TOC - chronological notebook entires with tags linked to TRACE elements/modelling tasks - can do this using Quarto blog

TRACE suggested tags:
* Model purpose; Research questions --> Problem formulation
* Model development; Design decisions --> Model description
* Parameterization; Patterns --> Data evaluation
* Conceptual design decisions --> Conceptual model evaluation
* Debugging; Software verification/Testing; Usability tools design --> Implementation verification
* Output verification/Goodness-of-fit; Calibration; Tests on environmental drivers --> Model output verification
* Sensitivity analysis; Uncertainty analysis; Robustness analysis; Simulation experiment --> Model analysis and application
* Output corroboration/Validation --> Model output corroboration

STRESS:

\section{Badges}

\textbf{TO DO:} Write this section, reference to appendices

\textbf{TO DO:} Compare criteria of badges? Might all be like kind of the same (e.g. open objects). If so, don't really need to choose between them, could just say meet all.

Table divides into five categories

Not relevant: pre-registration, replicated

Relevant: open objects, object review, reproduced - and hence, relevant badges to evaluate against could be:

\textbullet\ NISO "Open Research Objects (ORO)"\newline \textbullet\ ACM "Artifacts Available"\newline \textbullet\ COS "Open Data" and "Open Materials"\newline \textbullet\ IEEE "Code Available" and "Datasets Available"\newline \textbullet\ Springer Nature "Badge for Open Data"

\textbullet\ NISO "Research Objects Reviewed (ROR)"\newline \textbullet\ ACM "Artifacts Evaluated"\newline \textbullet\ IEEE "Code Reviewed" and "Datasets Reviewed"

\textbullet\ NISO "Results Reproduced (ROR-R)" \newline \textbullet\ IEEE "Code Reproducible" and "Dataset Reproducible" \newline \textbullet\ Psychological Science "Computational Reproducibility"
            
\section{Reporting guidelines}

Rough notes from Tom

Following the reproducibility test, we will assess the extent to which each paper follows the items recommended in:
The STRESS-DES reporting guidelines
The recent DES reporting guidelines published in Value in Health and based on ISPOR-SDM task force reports (Zhang et al, 2020)
We will score items as fully, partially, or not meeting the requirements.

\section{Reproduction test reports}

\textbf{TO DO:} Write this section. Need to reflect on whether it can consistute an RCR review or not. And what we want to do with it, or if just archived, or if plan to write up in some way or another.

Tom: Finally, we will map data from our best practice review, reporting guidelines adherence, and our tracked test data to success in badge allocation and the proportion of experiments reproduced.

\section{Versioning and Archiving}

This reproduction work will be made openly available throughout the project using GitHub. It will also be archived in Zenodo upon completion.

\textbf{TO DO:} Is there any exception here with the NHS Somerset work, in terms of how will work?

This protocol itself will also be archived as a pre-registration. Haroz 2022 identifies the Open Science Framework (OSF, \url{https://osf.io/}) and Zenodo (\url{https://zenodo.org/}) as suitable platforms for pre-registration.\autocite{haroz_comparison_2022} In this case, Zenodo will be used as this is where other materials already exist for the STARS project, and so it can be stored alongside them in a Zenodo "community".

\textbf{TO DO:} Maybe remove this section, and just mention abut GitHub and Zenodo above in record keeping and reports, and don't need to mention Haroz?

Rough notes from Tom:

Will use GitHub + Zenodo (automated via GitHub actions)
Will containerise with Docker or similar (again, automated via GitHub actions)
Will share via GitHub pages
MIT and CC-BY-4.0 licenses
Online book will describe how to reuse, adapt and reshare our outputs
