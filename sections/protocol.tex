\section{Introduction to the STARS project}

This protocol is part of the project STARS: ”Sharing Tools and Artefacts for Reusable Simulations in healthcare”. The aim of STARS is to...

\textbf{TO DO:} Write this section. Mention the aim, and refer to the key papers/pilot work, and then how that relates to this protocol (to set the scene). Keep it brief.

\autocite{monks_towards_2024}

\section{Selection of simulation models}

This protocol will be used within two work packages on STARS:
\begin{itemize}
    \item Work package 1 - to reproduce six published discrete event simulation models by external authors
    \item Work package 3 - to reproduce two simulation models produced by/in collaboration with members of the STARS team
\end{itemize}

\textbf{TO DO:} Remove mention of work packages, and just focus on the work being done (as work packages just relevant to us, not external) as that would make this clearer

For work package 1, the six models will be chosen from those identified by Monks and Harper 2023\autocite{monks_computer_2023} in their paper "Computer model and code sharing practices in healthcare discrete-event simulation: a systematic scoping review". Their review of discrete-event simulation models in healthcare identified 47 studies citing an openly available model that was used to generate their results. When selecting six of these models, we have two criteria: (i) The study has an open license (either already published, or added upon request from the STARS team), and (ii) That a balance of Python and R models are chosen.

For work package 3, the models will be developed within or with support from the STARS team, with a team member acting as a holdout to conduct the reproducibility test.

For WP1, email the authors. If email not active (e.g. rebounds) then search online to try and find most recent email. Email, if no response after 2 weeks then contact again.

Write this properly, base on and cite:
\begin{itemize}
    \item "PBR researchers email the corresponding author and at least one additional author (if applicable) to inform them that they will be conducting a PBR of their paper, include the PBR protocol,"\autocite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}
\end{itemize}

\section{Assessment of computational reproducibility}

The protocol for the reproduction of simulation models is adapted from several sources:
\begin{itemize}
    \item Berkeley Initiative for Transparency in the Social Sciences 2022 - "Guide for Advancing Computational Reproducibility in the Social Sciences"\autocite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}
    \item Wood et al. 2017 - "Replication protocol for push button replication"\autocite{wood_replication_2018} - as from their paper: "Push button replication: Is impact evaluation evidence for international development verifiable?"\autocite{wood_push_2018}
    \item Krafczyk et al. 2021 - "Learning from reproducing computational results: introducing three principles and the Reproduction Package"\autocite{krafczyk_learning_2021}
\end{itemize}

\subsection{Who does what}

\textbf{Rough notes:} One person responsible. They do everything below. Exception: Also involve other people for defining scope. Perhaps just needs one other person. They just have to do tasks of read article, and define scope. The two people do this independently, then come together to discuss, and choose consensus on scope. \textbf{TO DO:} Discuss this with project team, and whether they feel it is necessary

Need to record who does what in the records for it.

\subsection{Reading the article}

Download article and all supplementary and put in folder on GitHub that don't touch. Include appropriate citation and license in repository, based on article (e.g. if they MIT, then can do MIT. If they GPL, then GPL, etc.)

Read the article. As read through, there are two tasks to do:

\textbf{(1) Define scope of reproduction}
\begin{itemize}
    \item All tables and figures within the manuscript that contain results
    \item Any other key results in the paper (e.g. mentioned abstract, prominent in results)
\end{itemize}

Write this properly, base on and cite:
\begin{itemize}
    \item "PBR researchers select the key result and write a brief justification for why these are the main results highlighted in the paper. The result selected should have been presented as key by the authors, by, for example, being highlighted in the abstract, introduction, or conclusion.", "PBR studies should include all tables and any programme-generated figures included in the published paper. Researchers do not need to reproduce tables in the appendix." - from protocol\autocite{wood_replication_2018} of paper\autocite{wood_push_2018}
    \item ACRE - "define the scope of the reproduction by identifying the scientific claims and related display items that you will analyze in the remainder of the reproduction. For this exercise, we follow the a comparable definition of a claim as used in the SCORE project, a related initiative aimed at predicting replicability and reproducibility of research: “A research claim is a single major finding from a published study, as well as details of the methods and results that support this finding. A research claim [may not be] equivalent to an entire article. Sometimes the claim as described in the abstract does not exactly match the claim that is tested. In this case, you should consider the research claim to be that which is described in the [results of the paper]”" \cite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}
\end{itemize}

Do you produce the entire table and figure, or particular results from it? ACRE suggest it's individual within them. E.g. Figure - "record a number approximated from visual inspection of particular point in figure". I'm not sure about that approach, and presume we are thinking a bit more holistically about it.

Write the scope down somewhere (where). Can note down any tables and figures that aren't results and that won't reproduce. Can also note down any tables and figures that are in appendices that won't reproduce (unless they are considered "key results"). This key results thing resolves the issue of results being prominent in article but not in tables/figures, or results being in appendices but being important to reproduce.

For define scope, two people do it. Come together and discuss. Come to consensus. Then publicly archive this (point: can't back track, a bit like a pre-reg, think about where to put it).

Write this properly, base on and cite:
\begin{itemize}
    \item ACRE - process not necessarily linear except scope. "The only stage that should go first, and cannot be edited once finished, is the Scoping stage."\cite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}
\end{itemize}

\textbf{(2) Record key details about the methods} - note: can come back and make changes to this section, this is just trying to provide a structured way of getting started
\begin{itemize}
    \item Model create packages, software, versions (just from paper materials!)
    \item Model flow/diagram (if provided, or create if described)
    \item Data used
\end{itemize}

\subsection{Look at code and data} 

\textbf{To consider:} Do we fork their code repository or do we fork our template for working on reproducibility? Presuming latter as may not necessarily be on to fork?

Prior to working on this, in selection stage, will have checked at a glance whether there is code and stuff available for the model, that looks likely to be feasible, but not in any detail. Now start looking in more detail.

Download all there stuff and put in a folder on GitHub that don't touch.

Also make a second copy - that's where I will work on it and edit it.

Create a tree for their repository

Then a structured way of looking through it...
* Make a table recording all data files (name, location, one-sentence summary of what it is)
* Make a table recording all code files (name, location, one-sentence summary of what it does)

Then try and identify which sections of the code will produce your things from your scope

Figures will be compared by eye, but tables can compare with code. Will need table in usable format (i.e. CSV file). Will need to copy table into CSV as appropriate, so can then easily include and compare across. For figures, just want to have downloaded the figures from the paper as high quality as possible. Makes sense to do this AFTER have defined scope so don't copy over irrelevant tables or figures.

Write this properly, base on and cite:
\begin{itemize}
    \item "We downloaded any available material2 published with every paper"\cite{laurinavichyute_share_2022}
    \item "Examine any available code and data to associate those artifacts with corresponding computational experiments"\cite{krafczyk_learning_2021}
\end{itemize}

\subsection{Set up environment}

\textbf{Identifying requirements:}

To ensure a reproducible research environment, we need to know:
\begin{itemize}
    \item The operating system used (e.g. Windows, Mac, Linux) and its version 
    \item The software used (e.g. Python, R)
    \item The software packages used and their versions\autocite{the_turing_way_community_turing_2022}
\end{itemize}

However, it is likely that papers may not include all the information required (such as versions used). In which case:
\begin{itemize}
    \item No operating system - use Linux
    \item No operating system version - use closest to publication date
    \item No package versions - use closest to publication date
    \item No package list - look at the libraries/packages imported within the scripts
\end{itemize}

\textbf{Creating environment:}

One suggestion is to install packages and their dependencies at a point in time no later than publication date. Not all solutions below guarantee this though. The publication date is used as could have re-run code during approval process, but know not after publication.
\begin{itemize}
    \item Anecdotally, this could be particularly handy for R which often tries to get the most recent packages
\end{itemize}

Another suggestion is to match operating system to developer. Again, not all solutions below guarantee this. This will also be relevant later if want to test reproduction package on different operating systems.

\textbf{TO DO:} Evaluate these options. Consider whether they control OS. Consider whether they control date.

Python -
\begin{itemize}
    \item Conda, VirtualEnv, etc.
    \item Docker - to do development inside container, VSCode Dev Containers extension allows you to open folder inside container - \href{https://code.visualstudio.com/docs/devcontainers/containers}{source 1}
\end{itemize}

R -
\begin{itemize}
    \item Renv
    \item Posit Public Package Manager - can use Snapshot (earliest is Oct 2017, and 5 most recent versions of R), for Linux can install binary packages (which is much quicker, as usually R installs from source rather than binary unlike for Windows and Mac which makes it really slow) - \href{https://packagemanager.posit.co/client/#/repos/cran/setup}{source 1}, \href{https://docs.posit.co/faq/p3m-faq/#frequently-asked-questions}{source 2}
    \item Groundhog - can go back to R 3.2 and April 2015 (and apparently can patch to go earlier) - \href{https://www.brodrigues.co/blog/2023-01-12-repro_r/}{source 1}
    \item miniCRAN - \href{https://learn.microsoft.com/en-us/sql/machine-learning/package-management/create-a-local-package-repository-using-minicran?view=sql-server-ver16}{source 1}
    \item Docker - requires license for non-academic (e.g. NHS) use - but Podman can drop in as replacement. To do development inside a container isn't natively supported by RStudio but can use RStudioServer via Rocker. By default, it runs in ephemeral mode - any code created or saved is lost when close - but you can use volume argument to mount local folders - \href{https://towardsdatascience.com/running-rstudio-inside-a-container-e9db5e809ff8}{source 1}
\end{itemize}

\textbf{License:}

Update license if need to based on packages used for model. Put the Python and R instructions here on how to check that. License needs to be compatabile with authors paper AND with packages. If not able to use completely permissive license, add note explaining why (e.g. if built in RSimmer, will need to be GPL). Explain decision of license in README, and license file itself is located in the repository.

\subsection{Run code}

Try to run the code to produce the items from your scope.
Make it obvious where each item in the scope comes from (perhaps using Rmd/ipynb, perhaps with comments in the files that create it then Rmd to present, or something like that). If possible to do it all in a notebook that's nice as then you have code and stuff all in one place... but guessing might not always be practical...

\textbf{Consider:} Some people convert this into reproducibility package when they try and run it, and end up with the result being a Makefile that runs and produces everything. Do we do that now? Or do we save that for the reproduction package stage, and here, just focus on trying to get it to run and not reworking stuff too much? I think so.

Write this properly, base on and cite:
\begin{itemize}
    \item "Write scripts to run computational experiments and visualisation to produce figures and tables that matched or closely approximated those from original article" (part of 40h)\autocite{krafczyk_learning_2021}
\end{itemize}

Want to basically:
1. Run code, troubleshooting issues, and produce stuff from scope, and make sure it is clear where this is being produced (e.g. comments in code, or ipynb/Rmd with clear sections for each thing)
2. Compare against manuscript (using guidance below)... iterative (if notice differences, troublehsoot, etc).
3. Once done troubleshooting, select classification for each item in scope, and then overall for the paper, regarding reproduction success.

\subsection{Troubleshooting}

If the code does not run or is not producing the desired outcomes, researchers should attempt to troubleshoot. Examples of tasks:
\begin{itemize}
    \item Correct paths to files
    \item Correct versions of software, or missing packages/libraries
    \item Fixing errors in the code
\end{itemize}

If results obtained are incomplete or if it is not possible to run the code, after troubleshooting attempts, the research should contact the original author. This email should:
\begin{itemize}
    \item Recap of project (as will have emailed them before when started)
    \item Link to preliminary report with documented attempt and list of issues that require resolution, Make sure description of problem is specific (e.g. identifying line in paper and place in code where think something is missing, or where an issue is occuring)
    \item Ask for suggestions on alternative course of action for issues, or for the complete code/data if missing.
\end{itemize}

If no response in 2 weeks, try again. If no response in 4 weeks, can mark as no response.

When emailing authors, follow the guidance on language and adapt from the email templates provided by the "Guide for Accelerating Computational Reproducibility in the Social Sciences",\autocite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022} as in Appendix \ref{appendix:acre}.

\textbf{To consider:} Laurinavichyute et al. 2022 did not contact authors, since they consider reproducibility to be only about the available data and procedures and not anything shared privately.\autocite{laurinavichyute_share_2022}

If there is missing code with no response from authors, then can attempt to reproduce the figure based on paper by writing own code. \textbf{To Consider:} Do we allow this? Most don't, ACRE and Krafczyk do.

Write this properly, base on and cite:
\begin{itemize}
    \item "Run existing code when possible, adapt when missing, and write new code when examples not covered by received code"\autocite{krafczyk_learning_2021}
    \item Detailed instructions on how to contact authors from Wood, e.g. when emailing authors if don't respond in 2 weeks remind and if they ask for more than 3 months tell them if not provided within three months then project classified as no access.\autocite{wood_push_2018,wood_replication_2018}
    \item Likewise Hardwicke et al. 2021 - if no response after 2 weeks, try again, but no further after that, and allow max 2 months to resolve issues
    \item ACRE - recommend contacting authors, including file and line in script, and if it doesn't work, writing new code\autocite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}
    \item Konkol et al. 2018 - "If we encountered issues while running the scripts that we were unable to resolve by ourselves, we searched the Web for solutions. If this was unsuccessful, we contacted the corresponding author. If they did not reply within four weeks, we considered reproduction to have failed for this paper. Once the scripts compiled without issues, we did not further inspect the code or make any changes" \autocite{konkol_computational_2019}
\end{itemize}

\subsection{Time allowed}

\textbf{TO DO:} Discuss appropriate time allowance with team.

Whether have fixed time, or whether it's more subjective of keep going until just get stuck.
* More common not to have fixed time
* Some define stopping point that you decide upon
* If record time, need to decide on what include (e.g. if you need to convert table from paper into a usable format to compare against, is that included in time, presume so if you include reading time)
* Partly depends on how much distinction we have been reproducing results and creating reproduction package. I'm keen for the two to be fairly seperate TBH.

Krafczyk allow max 40 hours. They include:
* Running experiments
* Looking up references
* Emailing and communicating with authors
* Reading background material to understand, implement or fix code
They don't include creation of reproduction package, with restructuring of code and scripts\autocite{krafczyk_learning_2021}

Henderson set no time limit.\autocite{henderson_reproducibility_2024}

Laurinavichyute terminate if 20 or more values differ by more than 10 percent (not applicable to us as less value based).\autocite{laurinavichyute_share_2022}

ACRE give suggested proportions of time spent with different allowances of time (for assignments.... 2 weeks, 1 month or 1 semester... basically, spend more time on improvement and robustness if have more time, and scoping always 10 percent)

Krafczyk present time taken to produce each table/figure. Consider approach to this given:
* Won't do a yes/no for reproduction, but more nuanced info on it for each item
* Not just tables and figures - "key results" - which could be a table/figure from appendices, but could also be specific figures from the text.
* A study with more figures and tables will take longer (which is not presented in that figure as its percentage of them done)

Need to consider:
* Why we want to record time taken to produce stuff... what we hope to learn... etc Is it just time for the tables and figures? Or is it more about time taken on particular tasks? Or if table and figures, does it start when you start trying to run? etc.
* Why we want to limit overall time
To help guide choice here.

\subsection{Defining reproduction success...}

Success should be defined for each item in the scope.

\subsubsection{Detailed comparison of numbers, figures and tables against the manuscript...}

\textbf{Figures:} Visually compare figures with those from the manuscript, and identify any differences. Suggested categories of differences based on Konkol et al. 2019\autocite{konkol_computational_2019} are differences in "content" or in "design" that relate to: legend; labelling; results; aspect ratio; axes; placement; background.\autocite{konkol_computational_2019} \textbf{TO DO:} Consider what is of importance to us here.

\begin{itemize}
    \item Henderson et al. 2024 - compare graphs based on their numeric content and ignore differences in presentation\autocite{mcmanus_can_2019} - potentially, although presentation important eg stretched graph? although i guess that is more about conclusions than content?
\end{itemize}

\textbf{Numbers from text or tables:} The numbers and tables from the manuscript should be stored in the programming environment, either through defining the numbers in the script or by importing CSV files of the tables. These can then be directly compared against the numbers and tables produced by the script.

\begin{itemize}
    \item Schwander et al. 2021 - use mcmanus, looking for variation of 5\%, 10\% and 20\%\autocite{schwander_replication_2021} \textit{\textbf{potentially suitable}}
    \item Wood et al. 2017 - guidance for stat significance (not relevant for simulation); for parameters and coefficients they considered decision rule but then recognised meaningfulness of size of parameters varies across studies which precludes them from making single rule and so instruct research to use judgement but carefully document decisions. "the definition of what is a major/minor difference will be based on PBR researcher’sjudgment. If available, the researcher should use summary statistics, like mean values, as referenceswhenassessing the level of difference between the original publication and the PBR results". Suggests colour coding tables to indicate differences\autocite{wood_push_2018, wood_replication_2018} \textit{\textbf{good}, probably wouldn't colour code tables though}
    \item Hardwicke et al. 2021 \autocite{hardwicke_analytic_2021, hardwicke_pre-registered_2017}
\end{itemize}

\subsubsection{Labelling level of reproduction achieved for a given number/section, figure or table}

If binary, can do figure like Krafcyzk.

\begin{itemize}
    \item McManus et al. 2019 - identifical results; result varies by x\%; results vary by x\% and are consistent with original conclusion; figures produced to reasonable degree of success;\autocite{mcmanus_can_2019} - \textit{\textbf{not suitable.}not applicable to tables or figures (unless average percentage), require thinking about whether conclusions hold (which we are not assessing)}
\end{itemize}

\subsubsection{Labelling the overall level of reproduction achieved for the paper (considering all items in scope)}

Ways of doing this:
\begin{itemize}
    \item Henderson et al. 2024 - all reproduced; at least one reproduced;\autocite{henderson_reproducibility_2024} \textit{\textbf{not suitable}: requires binary decision on reproduction success for each, and not very informative}
    \item McManus et al. 2019 - various suggested definitions (table 2) - adapted examples that are not about a single result:  replicated for some scenarios and not others; same conclusions reached.\autocite{mcmanus_can_2019} \textit{\textbf{not suitable}: ambiguous}
    \item Laurinavichyute et al. 2022 - Strict criteria is that paper reproducible if all analyses could be reproduced exactly (except rounding errors). Relaxed criteria is if there are less than K cases (1, 5, 10, 20) where reproduced value differs by more than 10\% then paper is considered reproduced. Discprenancies smaller than 10\% ignored. K 1 corresponds to a single major discrepancny blocking reproducibility like Hardwicke. The upper threshold corresponds to small experiment being irreproducible - but paper reproducible in broad sense. \autocite{laurinavichyute_share_2022} \textit{\textbf{Relaxed criterion potentially relevant} - would need to consider how deal with figures - as this is focused on numbers - and could require converting figure to numbers or ignoring figures}
    \item Wood et al. 2017 - strongly advise against binary outcome, and instead label as: \textbf{comparable, minor differences, major differences}. "inability to replicate figures is of secondary concern and should be noted in the final report but should not generally influence replication status" \autocite{wood_push_2018, wood_replication_2018} \textit{\textbf{nice} but not sure about ignoring figures}. More detail on their categories:
    \begin{itemize}
        \item "Comparablereplication:identical results or very small changes(like rounding).
        \item Minor differences: small differences in coefficients and/or p-values.
        \item  Major differences:meaningful differences in reported outcomes (especially in the key results) or the code does not reproduce published results
        \item No access: the original authors do notreply or decline to provide data or code.
        \item Proprietary data: unable to provide data but provided replication code and DSL documentation.
        \item Incomplete: If PBR researchers are unable to reproduce part of the publication due to missing code and/or data, they will report to the original authors on the PBR study’s status and ask if the authors can provide more data or code to complete the PBR. If the PBR researchers cannot reproduce anytables after communicating with original authors, the paper will have two classifications. The paper will be classified as “incomplete” and in addition the paper will be classified based on the PBR results that were possible to run"
    \end{itemize}
\end{itemize}

\subsubsection{Conclusions}

Important to note as Laurinavichyute et al. 2022 do that "We do not evaluate whether the main claims of the study hold: identifying which results correspond to the main claims is a nuanced decision that is not always within our expertise."

\subsection{Keeping a log}

Keep a daily log of work. Very detailed. Open access.

All technical issues encountered should be documented. Konkol et al. 2018 - "We documented all technical issues and how we solved them. Each issue we encountered was categorised into one of the four categories: minor, substantial, severe, and system-dependent issues. All scripts that successfully compiled were then executed, and we saved all figures that they generated during execution" . Their issues:
\begin{itemize}
    \item Minor - library not found but available in repository; faulty variable call;
    \item Substantial - wrong directory; deprecated function; output not storable in local folder; function not found or missing library; library not found and not in repository; broken link;
    \item Severe - flawed functionality; missing data or code; flawed data integration; code in PDF;
    \item System-dependent - insufficient RAM; function behaves differently across operating systems; installing libraries on different operating systems\autocite{konkol_computational_2019}
\end{itemize}

\subsection{old notes}

Rough notes from Tom (Krafczyk):

Maximum 40 hours

1. Break tables and figures in a DES paper into a set of experiments. --> \textbf{DEFINE what is included using Wood}
2. Survey the model code and data provided with the publication. 
3. Modify and write code to run computational experiments. --> \textbf{refine this using Wood}
4. Automate computational experiments and visualizations. --> \textbf{further details from Krafczyk and Wood}
5. Restructure each step into a reproducible software package.
6. Track issues, barriers, and enablers to reuse and reproduction. --> \textbf{under record keeping}
7. Produce a plot reporting the percentage of experiments reproduced versus time. --> \textbf{under reporting, as this is just one figure that covers all of the paper}

To further support reproducibility, we will adopt a clear definition of replication success for a stochastic model using McManus et al (2021). We will investigate if conclusions remain the same allowing for multiple percent deviations from those reported (e.g 5, 10, 20 percent). --> \textbf{appraise this suggestion - this is more for papers where they are trying to get key figures - can have different rules for different types of output, as in examples in my notes}

We will adopt the same approach to measuring time (i.e. what activities are included and excluded) as used in Krafczyk et al (2021). --> \textbf{refine this using Wood}

Rough notes: TRACE modelling notebook (cav: designed for making models and then translating to TRACE reporting), STRESS reporting guidelines

TRACE:

TOC - chronological notebook entires with tags linked to TRACE elements/modelling tasks - can do this using Quarto blog

TRACE suggested tags:
* Model purpose; Research questions --> Problem formulation
* Model development; Design decisions --> Model description
* Parameterization; Patterns --> Data evaluation
* Conceptual design decisions --> Conceptual model evaluation
* Debugging; Software verification/Testing; Usability tools design --> Implementation verification
* Output verification/Goodness-of-fit; Calibration; Tests on environmental drivers --> Model output verification
* Sensitivity analysis; Uncertainty analysis; Robustness analysis; Simulation experiment --> Model analysis and application
* Output corroboration/Validation --> Model output corroboration

STRESS:

\section{Reproduction packages}

\textbf{TO DO:} Write this section

Look at:

Krafczyk reproduction package

ACM RCR replication package

Some notes from chat with rob and tom:
* runiverse freer than cran
* docker more for deployment/reproduction package... drop in replacement for docker, podman, as docker not FOSS, as if you work for NHS and use Docker you supposedly have to pay for license... check what is under license or not and if docker is just deksopt

\section{Badges}

\textbf{TO DO:} Write this section, reference to appendices

\textbf{TO DO:} Compare criteria of badges? Might all be like kind of the same (e.g. open objects). If so, don't really need to choose between them, could just say meet all.

Table divides into five categories

Not relevant: pre-registration, replicated

Relevant: open objects, object review, reproduced - and hence, relevant badges to evaluate against could be:

\textbullet\ NISO "Open Research Objects (ORO)"\newline \textbullet\ ACM "Artifacts Available"\newline \textbullet\ COS "Open Data" and "Open Materials"\newline \textbullet\ IEEE "Code Available" and "Datasets Available"\newline \textbullet\ Springer Nature "Badge for Open Data"

\textbullet\ NISO "Research Objects Reviewed (ROR)"\newline \textbullet\ ACM "Artifacts Evaluated"\newline \textbullet\ IEEE "Code Reviewed" and "Datasets Reviewed"

\textbullet\ NISO "Results Reproduced (ROR-R)" \newline \textbullet\ IEEE "Code Reproducible" and "Dataset Reproducible" \newline \textbullet\ Psychological Science "Computational Reproducibility"
            
\section{Reporting guidelines}

Rough notes from Tom

Following the reproducibility test, we will assess the extent to which each paper follows the items recommended in:
The STRESS-DES reporting guidelines
The recent DES reporting guidelines published in Value in Health and based on ISPOR-SDM task force reports (Zhang et al, 2020)
We will score items as fully, partially, or not meeting the requirements.

\section{Reproduction test reports}

\textbf{TO DO:} Write this section. Need to reflect on whether it can consistute an RCR review or not. And what we want to do with it, or if just archived, or if plan to write up in some way or another.

Tom: Finally, we will map data from our best practice review, reporting guidelines adherence, and our tracked test data to success in badge allocation and the proportion of experiments reproduced.

\section{Versioning and Archiving}

This reproduction work will be made openly available throughout the project using GitHub. It will also be archived in Zenodo upon completion.

\textbf{TO DO:} Is there any exception here with the NHS Somerset work, in terms of how will work?

This protocol itself will also be archived as a pre-registration. Haroz 2022 identifies the Open Science Framework (OSF, \url{https://osf.io/}) and Zenodo (\url{https://zenodo.org/}) as suitable platforms for pre-registration.\autocite{haroz_comparison_2022} In this case, Zenodo will be used as this is where other materials already exist for the STARS project, and so it can be stored alongside them in a Zenodo "community".

\textbf{TO DO:} Maybe remove this section, and just mention abut GitHub and Zenodo above in record keeping and reports, and don't need to mention Haroz?

Rough notes from Tom:

Will use GitHub + Zenodo (automated via GitHub actions)
Will containerise with Docker or similar (again, automated via GitHub actions)
Will share via GitHub pages
MIT and CC-BY-4.0 licenses
Online book will describe how to reuse, adapt and reshare our outputs

\section{Testing the protocol}

Test with Tom's paper.

Also maybe IPACS?

Think about what test and why. Is it run through of whole thing, or just particular bits.
