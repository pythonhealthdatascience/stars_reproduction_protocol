\section{Evaluation against guidelines}

\subsection{Badges}
\timeno

\hl{add review against code guidelines here, as that's essentially what the badges are.. OR just keep seperate}

\hlblue{\textbf{Grant:} Map results against ACM and NISO badges \textbf{Progress:} Collated more badge and journal examples, see Appendix, won't change approach much as ACM covers most (NISO is more of a suggestion than an official badging system), but can evaluate against all quite easily (lots of overlap) if appropriate}

\textbf{TO DO:} Write this section, reference to appendices

\textbf{TO DO:} Compare criteria of badges? Might all be like kind of the same (e.g. open objects). If so, don't really need to choose between them, could just say meet all.

Table divides into five categories

Not relevant: pre-registration, replicated

Relevant: open objects, object review, reproduced - and hence, relevant badges to evaluate against could be:

\textbullet\ NISO "Open Research Objects (ORO)"\newline \textbullet\ ACM "Artifacts Available"\newline \textbullet\ COS "Open Data" and "Open Materials"\newline \textbullet\ IEEE "Code Available" and "Datasets Available"\newline \textbullet\ Springer Nature "Badge for Open Data"

\textbullet\ NISO "Research Objects Reviewed (ROR)"\newline \textbullet\ ACM "Artifacts Evaluated"\newline \textbullet\ IEEE "Code Reviewed" and "Datasets Reviewed"

\textbullet\ NISO "Results Reproduced (ROR-R)" \newline \textbullet\ IEEE "Code Reproducible" and "Dataset Reproducible" \newline \textbullet\ Psychological Science "Computational Reproducibility"

\subsection{Code sharing guidelines}
\timeno

STARS

Any other major or relevant to DES.

\subsection{Reporting guidelines}
\timeno

\hlblue{\textbf{Grant:} Assess extent meets STRESS-DES and ISPOR-SDM reporting guidelines \textbf{Progress:} Not done yet. \textbf{To discuss though!:} Lots of examples doing similar work look also at code and compare against frameworks for how structure and share code - for example could compare against STARS - would we want to do that or not? Often used like context of "this didn't run" and "it didn't layout like we know is handy" type thing.}

Rough notes from Tom

Following the reproducibility test, we will assess the extent to which each paper follows the items recommended in:
The STRESS-DES reporting guidelines
The recent DES reporting guidelines published in Value in Health and based on ISPOR-SDM task force reports (Zhang et al, 2020)
We will score items as fully, partially, or not meeting the requirements.
