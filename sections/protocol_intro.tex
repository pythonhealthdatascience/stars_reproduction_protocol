\section{Introduction}

In this protocol, we are focused on the \textbf{computational reproducibility} of simulation models. This is defined as the ability to get consistent results with a prior study when using  the same data and methods as that study. We are focusing on models developed using \textbf{Python and R}, as these are popular free and open-source software (FOSS) for the development of models like discrete-event simulation (DES).\autocite{monks_computer_2023}

This protocol will first be used to reproduce six DES models with code available, selected from those identified by Monks and Harper 2023\autocite{monks_computer_2023} Selection of models has two criteria: (i) model is produced using Python and R, and a balance of Python and R models are chosen, and (ii) the study has an open license (either already published, or added upon request from the STARS team). The protocol will also be used to assess the computational reproducibility of two simulation models produced by/in collaboration with members of the STARS team.\hl{holdout for this? how deal? just say will be involved?}

Throughout the study, results will be openly available and shared via a Quarto website. This will compile information on the reproduction of the article. This includes the notebooks (.ipynb or .Rmd) producing the items in the scope, as well as a chronological log of work using Quarto blog posts, and then later, the reproduction report and detailed study results.

\hl{test protocol with toms paper and maybe ipacs? depends on how much test. think about what want to test and why. is it run through of whole thing or just particular bits. could make a note of what change following test run in appendices if think relevant to do so}

\vspace{2cm}
\section{Using this protocol}

When using this protocol, the researcher should work through the steps below in the \textbf{following order}:
\begin{itemize}
    \item First, work through \hyperref[sec:reproduce]{assessment of computational reproducibility}
    \item Then, in any order, evaluate study against \hyperref[sec:badges]{badges}, \hyperref[sec:artefacts]{recommendations on sharing research artefacts} and \hyperref[sec:reporting]{reporting guidelines}. This is conducted after the reproducibility assessment so as not to interfere with timings for reading and understanding the article and artefacts (as mentioned below).
    \item Finally, researchers should create the \hyperref[sec:compendium]{research compendium} and \hyperref[sec:report]{reproduction report}
\end{itemize}

Throughout all except the final two steps (research compendium and reproduction report), the researcher should keep a detailed record of work using a \textbf{logbook}. \hl{or do we want those last two in logbook?} This will be via through Quarto blog posts using the provided template (\url{https://github.com/pythonhealthdatascience/stars_reproduction_template}), which we set up for the study in one of the first steps of the computational reproducibility assessment. Each post in the log book should include the researcher name and date along with -
\begin{itemize}
    \item Comprehensive record of tasks, along with time spent (if applicable)
    \item Issues, barriers and enablers to reproduction
    \item Solutions to problems
    \item Timing and progress in reproducing each item in the scope (so we know what is completed when)
    \item Relevant links or links to files at that point (e.g. Git commit hash), with relevant files like the script and output files.
    \item Explanation of why things were done
    \item Notes on critical and non-critical issues to be addressed (such as in a to do list)
    \item If it makes more sense to include detailed descriptions elsewhere (such as in a script or notebook itself), then just be sure to link to that version of that file (such as via the Git commit history)
\end{itemize}

As suggested by Ayll√≥n et al. 2021\autocite{ayllon_keeping_2021} in their guidelines for keeping modelling notebooks, these posts will be daily, dated, chronological entries. Tags will be used to help indicate the activity on each day, and enable posts to be filtered by activity (although tags are free to be chosen by the researcher not related to a particular framework). Keeping a detailed log will support later understanding of what was done, and support preparing of final documents like the reproduction report.

The assessment of computational reproducibility is timed (with a few exceptions, as noted below alongside each task). These times should be recorded within the logbook alongside each activity (e.g. 12:10 to 12:45). The times should be monitored with a \textbf{maximum of 40 hours} allowed for attempting to reproduce the study, as in Krafczyk et al. 2021.\autocite{krafczyk_learning_2021} This cut-off is implemented as we anticipate there would be little more to learn from spending longer than that time on reproducing a single study.