\section{Assessment of computational reproducibility} \label{sec:reproduce}

\textbf{Remember!} Record progress in your logbook and the time spent on each task, as described above.

\vspace{0.5cm}
\subsection{Set-up}

\subsubsection{Inform the authors}

You may have already completed this step (as inform the authors about the study if emailing to request addition of an open license to the repository). If not, you should:
\begin{enumerate}
    \item \textbf{Email the corresponding author} to let them know about the study, including a \textbf{copy of/link to the study protocol}.
    \item If email rebounds, search online for a \textbf{recent email address} for any of the study authors.
\end{enumerate}

If you do not hear back from the study authors, you do not need to follow-up on this email asking for a response, as the email simply serves to notify the study authors and does not require a reply.

\vspace{0.5cm}
\subsubsection{Create repository using template}

To set up the repository:

\begin{enumerate}
    \item Go to \url{https://github.com/pythonhealthdatascience/stars_reproduction_template} and select the "\textbf{Use this template}" button.
    \item Set-up the repository within the 'pythonhealthdatascience' organisation using with a name following the format 'stars-reproduce-surname-year' and a description of 'Assessing the computational reproducibility of surname et al. year as part of STARS.' Here, 'surname' and 'year' refer to those for the article being reproduced.
\end{enumerate}

\vspace{0.5cm}
\subsubsection{Upload journal article and all artefacts to the repository}

 \textbf{Upload all available materials} from the article to the 'original\_study/' folder. This includes:
\begin{itemize}
    \item The \textbf{journal article} and any \textbf{supplementary materials}
    \item Each \textbf{table and figure} from the main journal article as individual digital objects (e.g. .jpeg, .png) (not including those from the appendices/supplementary materials)
    \item Any \textbf{code} and any related artefacts (for example, making a full copy of their GitHub repository)
\end{itemize}

In the logbook, include the \textbf{links} for where these uploaded materials were sourced from. You should also amend the provided \textbf{template page} ('quarto\_site/study\_publication.qmd') to display the PDFs for the journal article and supplementary material within the Quarto site, and provide a link to the code.

\subsubsection{Update license if required}

Check the type of license used by the study authors. By default, the template includes an MIT license (which is as permissive as possible), but \textbf{you may need to change this if the authors used a less permissive license}, so it is compatible with their license.

Following the guidance of the Turing Way\autocite{the_turing_way_community_turing_2022} and R packages book,\autocite{wickham_12_2023} the license does not need to be modified according to the packages used, unless code from that package is embedded within the work (for example, copy+and+paste a function) or if it is distributed as a binary with the work (i.e. bundled and stored with the work, rather than setting it to be exported from somewhere like PyPI or CRAN). If the code simply imports and uses functions from packages, this is not assumed to be derivative work, and hence a permissive license can be chosen.

\vspace{0.5cm}
\subsection{Scope of reproduction}

\subsubsection{Read the journal article}

\textbf{Read through the journal article} (but not yet looking into the code or data). If you want to take notes, include these within the logbook. These can be within a collapsible callout to aid readability of the log.

\vspace{0.5cm}
\subsubsection{Define scope of reproduction}

The next step is to define the scope of the reproducibility study - in other words, what parts of the paper you intend to reproduce. This should be focused on the \textbf{results of the simulation} (rather than other results like description of the sample. To identify the scope you should:

\begin{enumerate}
    \item Look through each of the \textbf{tables and figures} in the article (excluding the supplementary material) and identify whether they are within scope or not (i.e. do they present results of the simulation).
    \item Identify \textbf{'key results' in the text} of the article that are within scope. These are results that are highlighted within the \textbf{abstract or results} section. This may include items from the supplementary materials if referred to in the text.
    \item Evaluate whether the identified 'key results' are \textbf{already covered by the tables and figures} from the article. If not, they should be included in the scope.
    \item Make \textbf{consensus decision on scope with other team members}. Whilst the rest of this protocol is conducted by a single member of the STARS team, the scope of the reproducibility study will be defined as a consensus decision, with at least two team members reading the article and agreeing together on what is appropriate.
\end{enumerate}

You can include notes from thoughts and discussions around the potential scope within the \textbf{logbook}.

The criteria for what is considered part of the scope is adapted from Wood et al. 2018.\autocite{wood_replication_2018, wood_push_2018}

\vspace{0.5cm}
\subsubsection{Compile items in scope}

Once the scope has been decided, upload each item to the repository.

\begin{itemize}
    \item For \textbf{tables}, download a \textbf{CSV} version of each if available. Otherwise, convert the tables into CSV format.
    \item For \textbf{figures}, download the \textbf{highest-quality} version of each figure that is available. You should have already done this in an earlier step, but may still need to do this step if any figures are chosen from the supplementary materials as being in scope.
    \item For results described in the \textbf{text} (but not captured in a table or figure), record in a format appropriate to then later compare against (for example, within a \textbf{CSV}).
\end{itemize}

You should amend the provided \textbf{template page} ('evaluation/scope.qmd') to display each of the items in the scope.

\vspace{0.5cm}
\subsubsection{Archive scope on Zenodo}

With the organisation linked to Zenodo, \textbf{toggle Zenodo to preserve} that repository, and then create a \textbf{release on GitHub}, which Zenodo will then automatically download and register with a DOI. The release should be recorded within the Changelog.

This release should serve as a public registration of the intended scope of the reproducibility study, and archiving the repository at this point (prior to having started using or really looking at the the code). As stated in the "Guide for Accelerating Computational Reproducibility (ACRe) in the Social Sciences", it is important that the scope is defined at the start of the study, and publicly archived so as not to be amended during the course of the study.\cite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}

\vspace{0.5cm}
\subsection{Familiarise with artefacts}

\subsubsection{Look over the code/data}

\textbf{Browse through any code and any data} that you uploaded to the repository. The aim of this step is to familiarise with the materials before setting up the environment or running the code. There are no required steps for how you should do this, but some suggestions include:
\begin{itemize}
    \item In logbook, recording a one-sentence description of each file and tree of the uploaded files (as suggested by Ayll√≥n et al. 2021\autocite{ayllon_keeping_2021})
    \item Looking for sections of code that produce items within the scope and recording this within the logbook (as in Krafczyk et al. 2021\cite{krafczyk_learning_2021}).
\end{itemize}

\newpage
\subsection{Set up environment}

Identify the \textbf{software packages and their versions}, as used by the original study authors. This may be provided in an \textbf{environment file or within the article}. If not, the researcher should:
\begin{itemize}
    \item Identify packages from those named to \textbf{import} within each of the scripts
    \item Select versions by looking at the \textbf{version history} for that package on a package repository (e.g. PyPI, CRAN), and identifying a version whose release date that is \textbf{closest to but still prior to} the date of the code archive or paper publication (whichever is earliest).
\end{itemize}

Use or set up an environment file with the identified dependencies, and \textbf{create the environment}. Researchers should use simple methods for environment management such as Conda or VirtualEnv in Python, and renv in R. You do not need to match the operating system used.

\textbf{Remember!} This should be set up within the 'reproduction/' folder, whilst the 'original\_study/' folder should remain untouched. This means, if an environment file is provided, you should copy it into the 'reproduction/' folder.

\vspace{0.5cm}
\subsection{Attempt to reproduce items in scope}

This stage is an \textbf{iterative} process of running the code and attempting to reproduce the items in the scope. For this stage, the researcher should:
\begin{itemize}
    \item Leave the 'original\_study/' untouched - simply \textbf{copy over any relevant files} into the 'reproduction/' folder before running or modifying them.
    \item Use a \textbf{notebook} (.ipynb or .Rmd) when running the code, as this enables you to easily share the code and produced outputs from the scope, hence following a literate programming approach. This notebook can be made available to view within the Quarto site by setting it as part of the toctree in '\_quarto.yml', as in the template.
    \item Continue attempting to reproduce each item until you feel it is \textbf{successfully reproduced} (as detailed below) - or you run out of time (from the maximum 40 hours allowed).
    \item \textbf{Troubleshoot issues}, contacting the study authors if necessary (as detailed below)
\end{itemize}

\textbf{Remember!} Continue taking detailed notes and timings in your logbook, including each success and issue, and making note of any changes made to the model code. Whilst keeping notes in the logbook, it is recommended that you \textbf{copy over in-progress outputs} to the blog post folder (e.g. .png file for a figure), so that you can easily and visually share progress in reproduction

\vspace{0.5cm}
\subsubsection{Successful reproduction}

For \textbf{each item in the scope}, the researcher should decide whether it has been \textbf{successfully reproduced}. A binary decision should be made for each item (and none should be labelled as 'partial success').

This is a \textbf{subjective} decision. A successful reproduction does \textbf{not require that exactly the same results} are found. An item can be considered successfully reproduced if \textbf{minimal variation} is observed from the original results. For example, if the original study did not use seeds to completely control randomness, then this can be considered a level of variation that would be expected between different runs of the study. 

As an example, if it is possible to produce a table with some numbers being a match or very similar, but some numbers being substantially different, then this would be classed as having \textbf{not} been successfully reproduced. If however all aspects of the item were reproduced with reasonable similarity, this can be classed as \textbf{successful} reproduction.

Further recommendations:
\begin{itemize}
    \item \textbf{Figures} - these are compared \textbf{by eye}. Researchers should be unconcerned by \textbf{minor differences in presentation} (such as font), with regards to evaluating reproduction success.
    \item \textbf{Numbers} - researchers should calculate and report the \textbf{percentage difference} in results between the manuscript and the researcher. As reported by Wood et al. 2018,\autocite{wood_push_2018, wood_replication_2018} a meaningful difference in a value will vary between studies, and so it is difficult to set a single rule on what is or is not a minor difference. As such, researchers should follow a similar approach to Schwander et al. 2021,\autocite{schwander_replication_2021} considering whether the figure is reproducible at varying levels of percentage error (5\%, 10\% and 20\%). However, they \textbf{should then use their judgement to decide whether the item has been reproduced}. This is similar to one of the definitions proposed by McManus et al. 2019\autocite{mcmanus_can_2019} - "\textit{Results... vary only by XX\% compared to the original, AND are consistent with the original conclusions}" - incorporating both numerical comparison and allowance for variability in whether this constitutes a meaningful difference from the original results.
\end{itemize}

In assessing reproduction success, it is important to note (as in Laurinavichyute et al. 2022\autocite{laurinavichyute_share_2022} and Wood et al. 2018\autocite{wood_push_2018}) that the focus is not on the quality or robustness of the original results, or whether the main claims of the study are consistent. Instead, the focus is on whether it was possible to reproduce the article's results within a reasonable margin of error (given that we do expect a little variation, since discrete-event simulations are stochastic models, and may not have been fully controlled using random seeds or with any environment differences).

\hl{Should this be a consensus decision?}

\vspace{0.5cm}
\subsubsection{Troubleshooting}

Researchers should \textbf{troubleshoot} any issues encountered (including making changes to the provided code). In allowing modification and writing of code, our intention is that researchers try \textbf{as much as possible} to attempt to reproduce from the scope. The allowance of writing new code is similar to the approach of Krafczyk et al. 2021\autocite{krafczyk_learning_2021} and the ACRe project\autocite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}. Examples of changes you may need to make include:
\begin{itemize}
    \item Correcting paths to files
    \item Correcting the versions of software, or adding missing packages or libraries
    \item Fixing errors in the code
    \item Adding code to produce an item in the scope, if not otherwise provided
    \item Adding a method for controlling randomness in the simulation (if not otherwise set up), so you can get consistent results with yourself between re-runs of your notebook
\end{itemize}

Troubleshooting can include asking \textbf{advice from other members of the STARS team}. In these cases, the researcher should ensure that they include a record the time spent, everything discussed, and any recommendations made.

\vspace{0.5cm}
\subsubsection{Contacting the authors}

Despite troubleshooting, you may remain unable to run the code, or have large discrepancies with the original paper. In this case, you should \textbf{contact the original author}. This email should:
\begin{itemize}
    \item \textbf{Recap} of project (as will have emailed them before when started)
    \item Link to \textbf{preliminary repor}t with documented attempt and list of issues that require resolution, Make sure description of problem is specific (e.g. identifying line in paper and place in code where think something is missing, or where an issue is occuring)
    \item \textbf{Ask for suggestions} on alternative course of action for issues, or for the complete code/data if missing.
\end{itemize}

If there is no response in two weeks, the researcher should contact them again. If there is still no response two weeks later, this can be marked as no response. When emailing authors, it is suggested to follow the guidance on language and adapt from the \textbf{email template}s provided by the ACRe in the chapter "Guidance for Constructive Communication Between Reproducers and Original Authors" of their guide.\autocite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022} The allowance of contacting authors is similar to the approaches of several studies,\autocite{krafczyk_learning_2021,wood_push_2018,berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022,hardwicke_analytic_2021,konkol_computational_2019} with a maximum of four weeks for responses as in Konkol et al. 2018\autocite{konkol_computational_2019}. This approach does however differ from Laurinavichyute et al. 2022\autocite{laurinavichyute_share_2022} who did not contact authors, since they considered reproducibility to be only about the available data and procedures and not anything shared privately.\autocite{laurinavichyute_share_2022}

\vspace{0.5cm}
\subsection{Finishing up}

\subsubsection{Tidy up notebook and create reproduction success page}

Tidy the reproduction notebook, so it simply produces each of the items in the scope, and clearly states how each relates to the original article (e.g. captioning 'Reproduction attempt for Figure 2').

Using the template page ('evaluation/reproduction\_success.qmd'), show each item from the scope (as in the original article) alongside our best reproduction attempts. Include the decision on the reproduction success for each item (along with any justification for this decision).

\vspace{0.5cm}
\subsubsection{Archive on Zenodo}

Once all of the above is completed, you should archive the repository on Zenodo through creation of a new \textbf{GitHub release}. The repository should already be set up to sync with Zenodo from the earlier stage when we archived the scope, so this release will create an updated version of the archived repository, which will sit alongside the prior version. The release should be recorded within the Changelog.