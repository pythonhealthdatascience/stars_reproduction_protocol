\section{Assessment of computational reproducibility} \label{sec:reproduce}

\subsection{Set-up}

\subsubsection{Inform the authors}
\timeno

If the authors have not already been informed (such as via an email to request an open license is added to the repository), then the researcher should let the corresponding author know about this study. This email should inform the author that we will be conducting an assessment of the computational reproducibility of their study, and can include a copy of the protocol. If the email appears inactive due to rebounds, the researcher should search online to try and identify a recent email address for any of the study authors.

\subsubsection{Create repository using template}
\timeno

Use the STARS computational reproducibility template to create a new repository.

\begin{itemize}
    \item Template: \url{https://github.com/pythonhealthdatascience/stars_reproduction_template}. To use it, select the "Use this template" button.
    \item Organisation: \url{https://github.com/pythonhealthdatascience}
    \item Repository name: stars-reproduce-firstauthorsurname-publicationyear
    \item Description: Assessing the computational reproducibility of [firstauthor et al. publicationyear] as part of STARS.
    \item Public repository
\end{itemize}

\subsubsection{Upload journal article and all artefacts to the repository}
\timeyes

Upload all available materials from the article to the repository. This could include:
\begin{itemize}
    \item Journal article
    \item Supplementary materials
    \item Code (for example, copying the content of a GitHub repository)
    \item Protocol
\end{itemize}

\subsubsection{Select license}
\timeyes

Select a license for the repository. At this stage, this should be as permissive as possible, whilst being compatible with the license used by the study authors. For instructions on how to add a license to a repository, see \url{https://docs.github.com/en/communities/setting-up-your-project-for-healthy-contributions/adding-a-license-to-a-repository}.

\subsection{Scope of reproduction}

\subsubsection{Read the journal article}
\timeyes

Read through the journal article and supplementary materials (but not yet looking into the code or data). Consolidate this by adding a short description of the article to the Quarto book. This should be relatively concise, and cover the key methods and results. It might, for example, include any model flow diagram provided in the paper, any model parameters mentioned, and the model type and software. This can also be updated later.

\subsubsection{Make consensus decision on scope}
\timeyes[for scoping by primary researcher (but not including time of other researchers reading the article, or time spent discussing and coming to a consensus on the scope)]

The next step is to define the scope of the reproducibility study - in other words, what parts of the paper you intend to reproduce. The scope should include:
\begin{enumerate}[label=(\alph*)]
    \item All tables and figures within the manuscript that contain results from the simulation, and-
    \item Any other key results in the paper, that are not otherwise captured within the tables and figures. "Key results" can be considered things that are mentioned within the abstract or conclusion, or feature prominently within the results section.
\end{enumerate}

This does not need to include tables and figures in the Appendices (unless these are identified as "key results"). This method of defining the scope is based on that used by Wood et al. 2018.\autocite{wood_replication_2018, wood_push_2018}

Whilst the rest of this protocol is conducted by a single member of the STARS team, the scope of the reproducibility study will be defined as a consensus decision, with at least two team members reading the article and suggesting the scope independently, and then agreeing together on what is appropriate.

\subsubsection{Compile items in scope}
\timeyes

Once the scope has been decided, upload each item to the repository.

\begin{itemize}
    \item For tables, download a CSV version of each if available. Otherwise, convert the tables into CSV format.
    \item For figures, download the highest-quality version of each figure that is available. Then extract data from the figures. \hl{Do we want to do this? Do we trust the software?} Software available: \url{https://plotdigitizer.com/best-plot-digitizer}. Good options are:
    \begin{itemize}
        \item WebPlotDigitizer (\url{https://automeris.io/posts/}) - in active development, version 4 and below are GNU AGPL v3, version 5 (published 14 May 2024) and onwards will be closed source.
        \item Enguage Digitizer (\url{https://markummitchell.github.io/engauge-digitizer/}) - no further development since 3 years ago, GNU GPL v2
    \end{itemize}
    \item For results described in the text (but not captured in a table or figure), record in a format appropriate to then later compare against (for example, within a CSV).
\end{itemize}

\subsubsection{Archive scope and supporting documents on Zenodo}
\timeyes

Describe the scope within the Quarto book, providing justification for any "key results" chosen, and including reference to the uploaded figures and data for the items. Any tables and figures not being produced (for example, as they don't contain results from the simulation) should also be noted.

With the organisation linked to Zenodo, toggle Zenodo to preserve that repository, and then create a release on GitHub, which Zenodo will then automatically download and register with a DOI. This release should serve as a public registration of the intended scope of the reproducibility study, and archiving the repository at this point (prior to having started using or really looking at the the code).

As stated in the "Guide for Accelerating Computational Reproducibility (ACRe) in the Social Sciences", it is important that the scope is defined at the start of the study, and publicly archived so as not to be amended during the course of the study.\cite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}

\subsection{Familiarise with artefacts}

\subsubsection{Describe code/data}
\timeyes

Browse through any code and any data that you uploaded to the repository. In the Quarto book, add a tree of the uploaded files, along with one-sentence description of each file.

This is as suggested by Ayll√≥n et al. 2021\autocite{ayllon_keeping_2021} in their guidelines for keeping modelling notebooks

\subsubsection{Search for code that produces items in scope}
\timeyes

As in Krafczyk et al. 2021,\cite{krafczyk_learning_2021} look over the code and any data, and attempt to identify sections that correspond with items in the scope. Record this within the log.

\subsection{Set up environment}

\subsubsection{Identify dependencies}
\timeyes

To ensure a reproducible research environment, we need to know:
\begin{itemize}
    \item The operating system used (e.g. Windows, Mac, Linux) and its version 
    \item The software used (e.g. Python, R)
    \item The software packages used and their versions\autocite{the_turing_way_community_turing_2022}
\end{itemize}

However, it is likely that papers may not include all the information required (such as versions used). In which case:
\begin{itemize}
    \item No operating system - use Linux
    \item No operating system version - use closest to publication date \hl{Do we want to be this stringent?}
    \item No package list - look at the libraries/packages imported within the scripts
    \item No package versions - use closest to publication date
\end{itemize}

\subsubsection{Create environment}
\timeyes

\hl{Evaluate these options. Consider whether they control OS. Consider whether they control date. But also - whether we want that. How casual vs. standardised on this we want to be.}

One suggestion is to install packages and their dependencies at a point in time no later than publication date. Not all solutions below guarantee this though. The publication date is used as could have re-run code during approval process, but know not after publication.
\begin{itemize}
    \item Anecdotally, this could be particularly handy for R which often tries to get the most recent packages
\end{itemize}

Another suggestion is to match operating system to developer. Again, not all solutions below guarantee this. This will also be relevant later if want to test reproduction package on different operating systems.

Python -
\begin{itemize}
    \item Conda, VirtualEnv, etc.
    \item Docker - to do development inside container, VSCode Dev Containers extension allows you to open folder inside container - \href{https://code.visualstudio.com/docs/devcontainers/containers}{source 1}
\end{itemize}

R -
\begin{itemize}
    \item Renv
    \item Posit Public Package Manager - can use Snapshot (earliest is Oct 2017, and 5 most recent versions of R), for Linux can install binary packages (which is much quicker, as usually R installs from source rather than binary unlike for Windows and Mac which makes it really slow) - \href{https://packagemanager.posit.co/client/#/repos/cran/setup}{source 1}, \href{https://docs.posit.co/faq/p3m-faq/#frequently-asked-questions}{source 2}
    \item Groundhog - can go back to R 3.2 and April 2015 (and apparently can patch to go earlier) - \href{https://www.brodrigues.co/blog/2023-01-12-repro_r/}{source 1}
    \item miniCRAN - \href{https://learn.microsoft.com/en-us/sql/machine-learning/package-management/create-a-local-package-repository-using-minicran?view=sql-server-ver16}{source 1}
    \item Docker - requires license for non-academic (e.g. NHS) use - but Podman can drop in as replacement. To do development inside a container isn't natively supported by RStudio but can use RStudioServer via Rocker. By default, it runs in ephemeral mode - any code created or saved is lost when close - but you can use volume argument to mount local folders - \href{https://towardsdatascience.com/running-rstudio-inside-a-container-e9db5e809ff8}{source 1}
\end{itemize}

\subsubsection{Update license if necessary}
\timeyes

As above, we want to use a license that is as permissive as possible whilst being compatible with the license used by the author, as well as now the \textbf{licenses of any packages} used within the project. To identify the licenses used:
\begin{itemize}
    \item In Python, you could use the package "pip-licenses" (\url{https://pypi.org/project/pip-licenses/})
    \item In R, you can run the command \texttt{installed.packages(fields = "License")}
\end{itemize}

If it is not possible to use a completely permissive license like MIT, explain why (either due to author license or package licenses).

\subsection{Attempt to reproduce items in scope}

\subsubsection{Use provided code to produce notebook creating items in scope}
\timeyes[(excluding computation time at the researcher's discretion - for example, excluding time where a simulation is set to run for 5 minutes whilst the researcher makes a cup of tea)]

Run the code and attempt to produce the items in the scope. Insert the code into a \textbf{notebook} (.ipynb or .Rmd) as this enables you to easily share the code and produced outputs from the scope. Label each output clearly, as it relates to the manuscript. Whilst running this code, the researcher should be troubleshooting issues and evaluating reproduction success, as below.

When using their code, this should be kept separate from the unchanged copies of code that were downloaded at the start of the study. For example, copying over their script into a new notebook in a different folder (with separate folders for artefacts from the article, and for results produced by the researcher reproducing the article).

\subsubsection{Troubleshoot issues}
\timeyes[(including searching online, looking over artefacts, emailing the author, etc.)]

If there are difficulties in running the code or producing the desired outcomes, researchers should attempt to troubleshoot through changes to the environment or scripts. Examples of changes include:
\begin{itemize}
    \item Correcting paths to files
    \item Correcting the versions of software, or adding missing packages or libraries
    \item Fixing errors in the code
    \item Adding code to produce an item in the scope, if not otherwise provided
\end{itemize}

In allowing modification and writing of code, our intention is that researchers try \textbf{as much as possible} to attempt to reproduce from the scope. The allowance of writing new code is similar to the approach of Krafczyk et al. 2021\autocite{krafczyk_learning_2021} and the ACRe project\autocite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}.

If however, they remain unable to run the code, or have large discrepancies for any items, or any completely unable to reproduce any items, they should then \textbf{contact the original author}. This email should:
\begin{itemize}
    \item Recap of project (as will have emailed them before when started)
    \item Link to preliminary report with documented attempt and list of issues that require resolution, Make sure description of problem is specific (e.g. identifying line in paper and place in code where think something is missing, or where an issue is occuring)
    \item Ask for suggestions on alternative course of action for issues, or for the complete code/data if missing.
\end{itemize}

If there is no response in two weeks, the researcher should contact them again. If there is still no response two weeks later, this can be marked as no response. When emailing authors, it is suggested to follow the guidance on language and adapt from the \textbf{email template}s provided by the ACRe in the chapter "Guidance for Constructive Communication Between Reproducers and Original Authors" of their guide.\autocite{berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022}

The allowance of contacting authors is similar to the approaches of several studies,\autocite{krafczyk_learning_2021,wood_push_2018,berkeley_initiative_for_transparency_in_the_social_sciences_guide_2022,hardwicke_analytic_2021,konkol_computational_2019} with a maximum of four weeks for responses as in Konkol et al. 2018\autocite{konkol_computational_2019}. This approach does however differ from Laurinavichyute et al. 2022\autocite{laurinavichyute_share_2022} who did not contact authors, since they considered reproducibility to be only about the available data and procedures and not anything shared privately.\autocite{laurinavichyute_share_2022}

\subsubsection{Assess reproduction success}
\timeyes

For each item in the scope, the researcher should decide whether it has been successfully reproduced. This decision can be supported by calculating the percentage difference in results between the manuscript and the researcher. As reported by Wood et al. 2017,\autocite{wood_push_2018, wood_replication_2018} a meaningful difference in a value will vary between studies, and so it is difficult to set a single rule on what is or is not a minor difference. As such, researchers should follow a similar approach to Schwander et al. 2021,\autocite{schwander_replication_2021} considering whether the figure is reproducible at varying levels of percentage error (5\%, 10\% and 20\%). However, they should then use their judgement to decide whether the item has been reproduced. This is similar to one of the definitions proposed by McManus et al. 2019\autocite{mcmanus_can_2019} - "\textit{Results... vary only by XX\% compared to the original, AND are consistent with the original conclusions}" - incorporating both numerical comparison and allowance for variability in whether this constitutes a meaningful difference from the original results.

The researcher should not use a definition of "partial success". Instead, they should decide whether or not each item (e.g. table, figure, or section of text) was successfully reproduced or not as a whole (although can include a description to provide more nuance to that decision). As an example, if it is possible to produce a table with some numbers being a match or very similar, but some numbers being substantially different, then this would be classed as having \textbf{not} been successfully reproduced. If however all aspects of the item were reproduced with reasonable similarity, this can be classed as \textbf{successful} reproduction.

As in Henderson et al. 2024,\autocite{henderson_reproducibility_2024} figures are compared based on their numeric content. This can be based on numbers extracted from the figure (as below), or based on visually comparing the figures (in which case, percentage difference cannot be used to support any judgement of reproduction success). Researchers should be unconcerned by minor differences in presentation (such as font), with regards to evaluating reproduction success.

The overall reproduction success of the paper can be described as the proportion of items from the scope that were determined to be reproduced (e.g. 4 out of 5, 80\%). This provides more context than just stating that a paper was or was not reproduced. When describing the reproduction success, it is important to include context on troubleshooting steps required (such as writing new code or contacting the author)

In assessing reproduction success, it is important to note (as in Laurinavichyute et al. 2022\autocite{laurinavichyute_share_2022} and Wood et al. 2018\autocite{wood_push_2018}) that the focus is not on the quality or robustness of the original results, or whether the main claims of the study are consistent. Instead, the focus is on whether it was possible to reproduce the article's results within a reasonable margin of error (given that we do expect a little variation, since discrete-event simulations are stochastic models, and may not have been fully controlled using random seeds or with any environment differences).

\subsubsection{Archive on Zenodo}
\timeyes

Once all of the above is completed, you should archive the repository on Zenodo through creation of a new GitHub release. The repository should already be set up to sync with Zenodo from the earlier stage when we archived the scope, so this release will create an updated version of the archived repository, which will sit alongside the prior version.